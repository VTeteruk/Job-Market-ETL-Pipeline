/*
=====================================================
Job Market ETL Pipeline - Load CSV to Bronze Layer
=====================================================
Purpose:
    Loads raw job listing data from a CSV file into the
    'bronze.jobs' table using PostgreSQL's COPY command.

Context:
    - This is part of the Bronze Layer in a multi-stage ETL pipeline.
    - The CSV file is assumed to be generated by an upstream process
      (e.g., Airflow DAG) and stored in the Postgres-accessible path:
      /var/lib/postgresql/csv_files/<file_name>

Details:
    - The COPY command is optimized for fast bulk insertion.
    - It reads the header to map CSV fields correctly.
    - The 'created_at' column is automatically filled with
      the current timestamp (via default constraint).
    - Date fields (like 'posted_date') must follow the 'YYYY-MM-DD' format
      or be preprocessed in the ETL layer for compatibility.

Important:
    - Ensure that the CSV structure exactly matches the specified column order.
    - The 'job_id' must be unique (PRIMARY KEY).
    - The 'created_at' column is excluded from COPY input and handled by the database.

Usage:
    Replace <file_name> with the actual filename in the code (e.g., jobs_20250624.csv)
=====================================================
*/
COPY bronze.jobs (
    job_id, url, source, title, company, location, job_type,
    workplace_type, experience_level, description, salary,
    posted_date, applicant_count, view_count
)
FROM '/var/lib/postgresql/csv_files/<file_name>'
WITH (
    FORMAT csv,
    HEADER true,
    DELIMITER ',',
    QUOTE '"',
    ESCAPE '"'
);